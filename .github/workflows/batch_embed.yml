name: Batch Generate Embeddings for Historical Data

on:
  workflow_dispatch: # Manual trigger
    inputs:
      years:
        description: '要处理的年份列表 (例如: 2018,2019,2020)，留空表示处理所有年份'
        required: false
        type: string
        default: ''
      matrix_count:
        description: '要创建的并行矩阵数量 (推荐: 10-40)'
        required: false
        type: number
        default: 20

jobs:
  list_files_and_create_tasks:
    runs-on: ubuntu-latest
    outputs:
      task_matrices: ${{ steps.create_task_matrices.outputs.task_matrices }}
      matrix_count: ${{ steps.create_task_matrices.outputs.matrix_count }}
      total_tasks: ${{ steps.create_task_matrices.outputs.total_tasks }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # 只安装必要的依赖用于任务拆分，不包含torch和embed等重型依赖
          pip install huggingface_hub polars toml

      - name: List Parquet files, Models and Create Task Matrices
        id: create_task_matrices
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }} # Needs repo write access
          REPO_ID: lyk/ArxivEmbedding # Your dataset ID
          MATRIX_COUNT: ${{ github.event.inputs.matrix_count || 20 }} # 使用输入参数设置矩阵数量，默认20
          TARGET_YEARS: ${{ github.event.inputs.years }}
        run: |
          import os
          import sys
          from huggingface_hub import HfApi, hf_hub_download
          import toml
          import json
          import math
          import polars as pl
          from tqdm import tqdm
          from collections import defaultdict
          
          # 获取需要处理的年份列表（如果有指定）
          target_years_input = os.environ.get('TARGET_YEARS', '').strip()
          target_years = []
          if target_years_input:
              target_years = [year.strip() for year in target_years_input.split(',')]
              print(f"用户指定处理的年份: {target_years}")
          
          # List files from Hub
          api = HfApi()
          repo_files = api.list_repo_files(repo_id=os.environ['REPO_ID'], repo_type='dataset')
          # Filter for YYYY.parquet files (e.g., 2018.parquet, 2024.parquet)
          # Exclude current.parquet and non-parquet files
          year_files = [f for f in repo_files if f.endswith('.parquet') and f[:-len('.parquet')].isdigit()]
          
          # 如果指定了年份，只处理那些年份
          if target_years:
              year_files = [f for f in year_files if any(year in f for year in target_years)]
              
          print(f"Found yearly files to process: {year_files}")
          
          # 解析配置文件中的模型
          config = toml.load("config.toml")
          embedding_configs = {}
          
          # 打印配置文件结构以便调试
          print("配置文件结构:")
          for key, value in config.items():
              print(f"  {key}: {type(value)}")
              if isinstance(value, dict):
                  for subkey in value.keys():
                      print(f"    - {subkey}")
          
          # 正确处理[Embedding.XXX]格式
          if "Embedding" in config and isinstance(config["Embedding"], dict):
              embedding_section = config["Embedding"]
              for key, value in embedding_section.items():
                  embedding_configs[key] = value
                  print(f"找到模型配置: {key} -> {value}")
          
          model_keys = list(embedding_configs.keys())
          
          if not model_keys:
              print("错误: 无法从config.toml中找到模型配置。")
              sys.exit(1)
          
          print(f"Found model keys: {model_keys}")
          print(f"Models with configurations:")
          for k, v in embedding_configs.items():
              print(f"  - {k}: {v}")
          
          # 新的基于ID的任务分配方法
          # 收集所有年份文件中的ID
          all_ids = []
          year_id_map = {}  # 记录每个ID属于哪个年份文件

          print("开始下载并读取所有年份文件的ID...")
          for year_file in tqdm(year_files, desc="读取文件ID"):
              try:
                  # 下载年份文件
                  file_path = hf_hub_download(
                      repo_id=os.environ['REPO_ID'],
                      filename=year_file,
                      repo_type="dataset",
                      local_dir=".",
                  )
                  
                  # 只读取ID列，减少内存使用
                  df = pl.read_parquet(file_path, columns=["id"])
                  file_ids = df["id"].to_list()
                  
                  # 记录ID与年份文件的对应关系
                  for id_value in file_ids:
                      year_id_map[id_value] = year_file
                  
                  # 收集所有ID
                  all_ids.extend(file_ids)
                  
                  # 清理下载的文件
                  os.remove(file_path)
                  
                  print(f"已从 {year_file} 读取 {len(file_ids)} 个ID")
              except Exception as e:
                  print(f"读取 {year_file} 时出错: {e}")
          
          # 去重
          unique_ids = list(set(all_ids))
          print(f"Total unique IDs across all files: {len(unique_ids)}")
          
          # --- Filter tasks: Check for existing embeddings --- 
          print("开始检查已存在的嵌入...")
          needed_tasks = []
          tasks_to_check_by_year = defaultdict(list)

          # Group potential tasks by year file first
          for id_value in unique_ids:
              year_file = year_id_map.get(id_value)
              if year_file:
                  for model_key in model_keys:
                      tasks_to_check_by_year[year_file].append({"id": id_value, "model_key": model_key})
              else:
                   print(f"警告：未找到ID {id_value} 对应的年份文件（在检查阶段），跳过。")

          processed_year_files = set()
          for year_file, potential_tasks in tqdm(tasks_to_check_by_year.items(), desc="检查年份文件"):
               try:
                   print(f"检查文件: {year_file}")
                   # Download year file ONCE per year
                   file_path = hf_hub_download(
                       repo_id=os.environ['REPO_ID'],
                       filename=year_file,
                       repo_type="dataset",
                       local_dir=".",
                   )
                   
                   # Load dataframe and schema
                   df_year = pl.read_parquet(file_path)
                   schema = df_year.schema
                   print(f"  文件 {year_file} schema: {list(schema.keys())}")

                   # Check tasks for this year
                   ids_in_file = set(df_year["id"].to_list()) # For quick ID existence check
                   for task in potential_tasks:
                       model_key = task["model_key"]
                       id_value = task["id"]

                       # Basic check: Does the ID exist in this year file?
                       if id_value not in ids_in_file:
                            print(f"  警告：ID {id_value} 在文件 {year_file} 中未找到，跳过任务 {model_key}")
                            continue
                            
                       # Check 1: Does the embedding column exist?
                       if model_key not in schema:
                           # Column doesn't exist, need to generate
                           needed_tasks.append({"id": id_value, "year_file": year_file, "model_key": model_key})
                           # print(f"  需要任务: {id_value} - {model_key} (列不存在)")
                       else:
                           # Column exists, check if the value for this ID is null
                           embedding_value = df_year.filter(pl.col("id") == id_value).select(pl.col(model_key)).item()
                           # Polars returns None for null values in list columns when using item()
                           if embedding_value is None:
                               needed_tasks.append({"id": id_value, "year_file": year_file, "model_key": model_key})
                               # print(f"  需要任务: {id_value} - {model_key} (值为空)")
                           # else: # Value exists, skip task
                           #     print(f"  跳过任务: {id_value} - {model_key} (值已存在)")

                   # Clean up downloaded file
                   os.remove(file_path)
                   processed_year_files.add(year_file)

               except Exception as e:
                   print(f"检查 {year_file} 时出错: {e}。将假设该文件所有任务都需要处理。")
                   # Fallback: If check fails, assume all potential tasks for this year are needed
                   for task in potential_tasks:
                        needed_tasks.append({"id": task["id"], "year_file": year_file, "model_key": task["model_key"]})
                   # Attempt cleanup even on error
                   if 'file_path' in locals() and os.path.exists(file_path):
                       try: os.remove(file_path) 
                       except OSError: pass

          total_potential_tasks = len(unique_ids) * len(model_keys)
          total_needed_tasks = len(needed_tasks)
          print(f"检查完成。总潜在任务数: {total_potential_tasks}。实际需要处理的任务数: {total_needed_tasks}")
           
          # 根据矩阵数量均匀分配ID
          target_matrix_count = int(os.environ['MATRIX_COUNT'])
          # limit_per_matrix = 10 # Limit tasks per matrix for testing
          # print(f"测试模式：每个矩阵最多处理 {limit_per_matrix} 个任务。")
          
          # 计算合理的矩阵数量 - 基于需要的任务和限制
          if total_needed_tasks == 0:
               actual_matrix_count = 0 # No tasks, no matrices
               print("警告：没有找到任何需要处理的任务。")
          else:
              # --- 修改: 不再使用固定 limit_per_matrix ---
              # Calculate the number of tasks per matrix, aiming for the target_matrix_count
              # If fewer tasks than matrices, each matrix gets at least one task
              tasks_per_matrix = math.ceil(total_needed_tasks / target_matrix_count)
              # Ensure at least one task per matrix if there are tasks
              tasks_per_matrix = max(1, tasks_per_matrix) 
              
              # Calculate the actual number of matrices needed based on the calculated tasks_per_matrix
              # This ensures we don't create empty matrices if tasks_per_matrix is large
              actual_matrix_count = math.ceil(total_needed_tasks / tasks_per_matrix)
              # Respect the user's requested matrix count as an upper limit, but ensure at least 1 matrix if tasks exist
              actual_matrix_count = min(target_matrix_count, actual_matrix_count)
              actual_matrix_count = max(1, actual_matrix_count)
              
              # Recalculate tasks_per_matrix based on the final actual_matrix_count
              # This distributes tasks as evenly as possible among the *actual* matrices being used.
              tasks_per_matrix = math.ceil(total_needed_tasks / actual_matrix_count)

              print(f"总任务数: {total_needed_tasks}。目标矩阵数: {target_matrix_count}。实际使用矩阵数: {actual_matrix_count}。计算得到每个矩阵约处理: {tasks_per_matrix} 个任务。")
              # --- 修改结束 ---

          task_matrices_for_strategy = [] # Keep this for generating matrix IDs
          
          if actual_matrix_count > 0:
              # Distribute *needed_tasks* among the matrices
              
              # 创建任务文件
              os.makedirs("matrix_tasks", exist_ok=True) # Directory to hold task files
              
              assigned_task_count_total = 0
              tasks_to_distribute = needed_tasks # Use the filtered list
              # Shuffle tasks to get a more random sample if desired
              # import random
              # random.shuffle(tasks_to_distribute)

              for i in range(actual_matrix_count):
                  # --- 修改: 使用计算出的 tasks_per_matrix ---
                  # Assign tasks to this matrix
                  start_idx = i * tasks_per_matrix
                  end_idx = min(start_idx + tasks_per_matrix, total_needed_tasks)
                  # --- 修改结束 ---
                  
                  # Slice the needed tasks for this matrix
                  matrix_tasks = tasks_to_distribute[start_idx:end_idx]

                  # Save tasks for this matrix to a JSON file
                  matrix_task_file = f"matrix_tasks/matrix_{i}_tasks.json"
                  with open(matrix_task_file, "w") as f:
                      json.dump(matrix_tasks, f)
                  
                  assigned_task_count_total += len(matrix_tasks)                  
                  print(f"矩阵 {i}: 分配了 {len(matrix_tasks)} 个任务 (从索引 {start_idx} 到 {end_idx-1})，保存到 {matrix_task_file}")
                  task_matrices_for_strategy.append({"matrix_id": i}) # Add matrix ID for strategy

                  # Stop assigning if we've run out of tasks (shouldn't happen with ceil division)
                  if end_idx >= total_needed_tasks:
                      break 
              
              # Recalculate actual_matrix_count based on how many matrices actually got tasks
              # This handles the edge case where the calculation might slightly overestimate due to ceil
              actual_matrix_count = len(task_matrices_for_strategy)
              print(f"总共分配了 {assigned_task_count_total} 个任务到 {actual_matrix_count} 个矩阵文件中。")

          # 输出到GitHub Actions
          task_matrices_json = json.dumps(task_matrices_for_strategy) # Output list of matrix IDs for strategy
          matrix_count = actual_matrix_count # Output the actual count used

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"task_matrices={task_matrices_json}\n")
              f.write(f"total_tasks={total_needed_tasks}\n")
              f.write(f"matrix_count={matrix_count}\n") # Add matrix_count output
          
          # 上传任务信息 (optional, for debugging)
          with open("all_tasks_info.json", "w") as f:
              all_tasks_info = {
                  "total_unique_ids": len(unique_ids),
                  "total_models": len(model_keys),
                  "total_tasks": total_needed_tasks,
                  "target_matrices_requested": target_matrix_count,
                  "actual_matrices_created": actual_matrix_count
              }
              json.dump(all_tasks_info, f)
        shell: python # Use python shell directly
          
      - name: Upload individual task files
        if: steps.create_task_matrices.outputs.matrix_count > 0 # Only upload if matrices were created
        uses: actions/upload-artifact@v4
        with:
          name: matrix-tasks # A single artifact containing all task files
          path: matrix_tasks/
          retention-days: 1 # Keep task files for 1 day

      - name: Upload task summary info
        uses: actions/upload-artifact@v4
        with:
          name: task-summary-info
          path: all_tasks_info.json
          retention-days: 1

  process_task_matrix:
    needs: list_files_and_create_tasks
    if: needs.list_files_and_create_tasks.outputs.matrix_count > 0 # Only run if matrices exist
    runs-on: ubuntu-latest # Consider GPU runners if available: ubuntu-latest-gpu
    strategy:
      fail-fast: false # Continue other jobs even if one fails
      matrix:
        matrix_info: ${{ fromJson(needs.list_files_and_create_tasks.outputs.task_matrices) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Get pip cache dir
        id: pip-cache
        run: |
          echo "dir=$(pip cache dir)" >> $GITHUB_OUTPUT

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ${{ steps.pip-cache.outputs.dir }}
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Use --extra-index-url so pip checks PyPI *and* the PyTorch CPU index
          pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cpu
          # Add accelerate if using multi-gpu or specific optimizations
          # pip install accelerate
          # Add GPU specific libraries if using GPU runner
          # pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Example for CUDA 11.8

      - name: Download tasks for this matrix
        uses: actions/download-artifact@v4
        with:
          name: matrix-tasks # Name of the artifact containing task files
          path: matrix_tasks # Download to a local directory

      - name: Process Tasks using Python Script
        id: process_tasks
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
          REPO_ID: lyk/ArxivEmbedding
          MATRIX_ID: ${{ matrix.matrix_info.matrix_id }} # Get matrix_id from strategy
        run: |
          # Define matrix_id and task_file path based on the strategy
          MATRIX_ID=${{ matrix.matrix_info.matrix_id }}
          TASK_FILE="matrix_tasks/matrix_${MATRIX_ID}_tasks.json"
          OUTPUT_DIR="matrix_output_${MATRIX_ID}"

          # Create output directory
          mkdir -p $OUTPUT_DIR

          echo "Processing Matrix ID: $MATRIX_ID"
          echo "Task File: $TASK_FILE"
          echo "Output Directory: $OUTPUT_DIR"

          # Run the new Python script
          python script/process_matrix_tasks.py \
            --matrix-id $MATRIX_ID \
            --task-file $TASK_FILE \
            --output-dir $OUTPUT_DIR \
            --config-file config.toml \
            --repo-id $REPO_ID \
            --batch-size 4 \
            --engine torch \
            --device cpu # Use "cuda" for GPU

      - name: Upload matrix output artifacts
        uses: actions/upload-artifact@v4
        with:
          name: matrix-output-${{ matrix.matrix_info.matrix_id }} # Use matrix_id from strategy
          path: matrix_output_${{ matrix.matrix_info.matrix_id }} # Use matrix_id from strategy
          retention-days: 5 # Keep artifacts for 5 days

  merge_and_upload:
    needs: [list_files_and_create_tasks, process_task_matrix]
    # Run only if the process_task_matrix jobs were triggered (i.e., matrix_count > 0)
    # and ran successfully, or if matrix_count was 0 initially.
    # We need to be careful here. Let's run it always if list_files ran,
    # but the inner script should handle the case where no artifacts are downloaded.
    if: always() && needs.list_files_and_create_tasks.result == 'success'
    runs-on: ubuntu-latest
    # This job runs once after all matrix jobs are done (or if none were needed)
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install polars huggingface_hub tqdm loguru

      - name: Download all matrix output artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts # Download all artifacts into 'artifacts' directory
          
      - name: Collect and organize all embedding outputs
        run: |
          # Create a directory to collect all embedded files
          mkdir -p all_embedded_files
          
          # Find all embedded-*.parquet files across all matrix outputs
          find artifacts/matrix-output-* -name "embedded-*.parquet" -exec cp {} all_embedded_files/ \;
          
          # List what we found
          echo "Found the following embedded files:"
          ls -la all_embedded_files/

      - name: Merge Embeddings and Upload
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
          REPO_ID: lyk/ArxivEmbedding
        run: |
          import os
          import glob
          import polars as pl
          from huggingface_hub import HfApi, hf_hub_download, upload_file
          from tqdm import tqdm
          from loguru import logger
          import re

          repo_id = os.environ['REPO_ID']
          api = HfApi()

          # Identify unique year files from the embedded file patterns
          embedded_files = glob.glob("all_embedded_files/embedded-*.parquet")
          year_files_to_process = set()
          
          # Extract year from filenames like embedded-2020.parquet-model_key.parquet
          for file_path in embedded_files:
              filename = os.path.basename(file_path)
              # Extract the original year file name
              match = re.search(r'embedded-([0-9]+\.parquet)', filename)
              if match:
                  year_file = match.group(1)
                  year_files_to_process.add(year_file)

          logger.info(f"Found year files to process: {year_files_to_process}")

          for year_file in tqdm(list(year_files_to_process), desc="Merging and Uploading Years"):
              logger.info(f"--- Processing {year_file} ---")
              local_final_path = f"final_{year_file}"
              
              # Find all embedding files for this year
              embedding_files = [f for f in embedded_files if f"embedded-{year_file}-" in os.path.basename(f)]
              
              if not embedding_files:
                  logger.warning(f"No embedding artifacts found for {year_file}. Skipping.")
                  continue

              # Download the original file from the Hub to merge into
              try:
                  logger.info(f"Downloading original {year_file} from Hub...")
                  original_file_path = hf_hub_download(repo_id=repo_id, filename=year_file, repo_type='dataset', local_dir='.')
                  logger.info(f"Loading original file: {original_file_path}")
                  # Ensure 'id' column is read as string/object if necessary
                  # Let Polars infer for now, but be mindful if IDs cause issues
                  df_merged = pl.read_parquet(original_file_path)
                  logger.info(f"Original {year_file} shape: {df_merged.shape}")
                  
                  # Store original columns and dtypes for later verification
                  original_schema = df_merged.schema
                  logger.debug(f"Original schema: {original_schema}")

                  # Ensure the 'id' column exists
                  if 'id' not in df_merged.columns:
                      logger.error(f"Original file {year_file} is missing the required 'id' column. Skipping merge.")
                      if os.path.exists(original_file_path):
                           os.remove(original_file_path)
                      continue # Skip to the next year file

                  os.remove(original_file_path) # Clean up downloaded original
              except Exception as e:
                  logger.error(f"Failed to download or load original {year_file} from Hub: {e}. Skipping merge for this year.")
                  # Clean up potentially downloaded file if path exists
                  if 'original_file_path' in locals() and os.path.exists(original_file_path):
                      try:
                          os.remove(original_file_path)
                      except OSError:
                           pass # Ignore error if removal fails (e.g., file not found)
                  continue

              # Merge embedding columns from artifacts
              merged_cols_count = 0
              processed_ids_for_year = set() # Track IDs processed in this year's merge

              for embed_file in embedding_files:
                  try:
                      logger.debug(f"Loading embedding file: {embed_file}")
                      df_embed = pl.read_parquet(embed_file)
                      logger.debug(f"Embedding file {embed_file} shape: {df_embed.shape}, columns: {df_embed.columns}")

                      # Ensure 'id' column exists in embedding file
                      if 'id' not in df_embed.columns:
                          logger.warning(f"Embedding file {embed_file} is missing 'id' column. Skipping.")
                          continue

                      # Find the embedding column (assume it's the model_key used in the filename or the only non-'id' column)
                      potential_embed_cols = [col for col in df_embed.columns if col != 'id']

                      if not potential_embed_cols:
                          logger.warning(f"No potential embedding columns (other than 'id') found in {embed_file}. Skipping.")
                          continue

                      # Heuristic: Assume the first non-'id' column is the embedding column
                      # Or better: extract model key from filename if possible
                      embed_col_name = None
                      model_match = re.search(r'embedded-.*?-([a-zA-Z0-9_.-]+)\.parquet$', os.path.basename(embed_file))
                      if model_match:
                           extracted_model_key = model_match.group(1)
                           if extracted_model_key in df_embed.columns:
                               embed_col_name = extracted_model_key
                               logger.debug(f"Identified embedding column '{embed_col_name}' from filename {embed_file}")
                           else:
                               logger.warning(f"Extracted model key '{extracted_model_key}' not found as column in {embed_file}. Falling back to heuristic.")

                      if not embed_col_name:
                           if len(potential_embed_cols) == 1:
                               embed_col_name = potential_embed_cols[0]
                               logger.debug(f"Using the only non-id column '{embed_col_name}' as embedding column from {embed_file}")
                           else:
                               logger.warning(f"Could not reliably determine the embedding column in {embed_file} (found: {potential_embed_cols}). Skipping file.")
                               continue

                      # Check if this embedding column should actually be merged
                      if embed_col_name in df_merged.columns:
                           logger.warning(f"Column '{embed_col_name}' already exists in the merged DataFrame for {year_file}. Skipping merge from {embed_file}.")
                           continue # Skip this file as the column exists

                      logger.debug(f"Attempting to merge column '{embed_col_name}' from {embed_file}")

                      # Select only id and the embedding column
                      # Ensure 'id' type matches the main dataframe if possible
                      try:
                          target_id_type = df_merged['id'].dtype
                          df_embed_subset = df_embed.select(['id', embed_col_name]).with_columns(pl.col('id').cast(target_id_type))
                      except Exception as cast_err:
                           logger.warning(f"Could not cast 'id' column in {embed_file} to match target type {target_id_type}. Proceeding with original type. Error: {cast_err}")
                           df_embed_subset = df_embed.select(['id', embed_col_name])


                      # Perform the left join
                      df_merged_before_join = df_merged.shape
                      df_merged = df_merged.join(df_embed_subset, on='id', how='left')
                      df_merged_after_join = df_merged.shape

                      if df_merged_after_join[0] != df_merged_before_join[0]:
                          logger.warning(f"Join changed row count for {year_file} when merging {embed_col_name} (before: {df_merged_before_join[0]}, after: {df_merged_after_join[0]}). This might indicate duplicate IDs.")
                          # Optional: Add check here to revert or handle duplicates if necessary

                      # Verify the column was added
                      if embed_col_name in df_merged.columns:
                           null_count = df_merged[embed_col_name].is_null().sum()
                           logger.info(f"Merged column '{embed_col_name}' ({df_merged_after_join[1]} total columns). Null count: {null_count}/{df_merged_after_join[0]}.")
                           merged_cols_count += 1
                           # Track processed IDs for this specific merge
                           processed_ids_for_year.update(df_embed_subset['id'].unique().to_list())
                      else:
                           logger.error(f"Column '{embed_col_name}' was NOT added after the join operation from {embed_file}. Shape after join: {df_merged_after_join}")


                  except Exception as e:
                      logger.error(f"Failed to process or merge embedding file {embed_file}: {e}", exc_info=True)

              # After merging all files for the year
              if merged_cols_count > 0:
                  # Final check on shape and schema
                  logger.info(f"Finished merging for {year_file}. Final shape: {df_merged.shape}. Merged {merged_cols_count} new columns.")
                  logger.debug(f"Final schema for {year_file}: {df_merged.schema}")
                  
                  # Optional: Validate that original columns still exist and types match
                  for col_name, col_type in original_schema.items():
                       if col_name not in df_merged.columns:
                            logger.error(f"Validation Error: Original column '{col_name}' missing in final merged data for {year_file}!")
                       elif df_merged[col_name].dtype != col_type:
                            logger.warning(f"Validation Warning: Original column '{col_name}' dtype changed in final merged data for {year_file} (original: {col_type}, final: {df_merged[col_name].dtype}).")


                  # Save locally before upload
                  try:
                      logger.info(f"Saving final merged file locally: {local_final_path}")
                      df_merged.write_parquet(local_final_path, compression='zstd')

                      # Upload the final merged file
                      logger.info(f"Uploading final {year_file} to Hub...")
                      upload_file(
                          path_or_fileobj=local_final_path,
                          path_in_repo=year_file, # Overwrite the file in the repo root
                          repo_id=repo_id,
                          repo_type="dataset",
                          commit_message=f"Add/Update embeddings for {year_file}"
                      )
                      logger.info(f"Successfully uploaded updated {year_file}.")
                  except Exception as e:
                      logger.error(f"Failed to save or upload final {year_file}: {e}")
                  finally:
                      if os.path.exists(local_final_path):
                          os.remove(local_final_path) # Clean up local final file
              else:
                  logger.warning(f"No new embedding columns were merged for {year_file}. Nothing to upload.")

          logger.info("--- Merge and Upload Job Finished ---")
        shell: python