name: Batch Generate Embeddings for Historical Data

on:
  workflow_dispatch: # Manual trigger
    inputs:
      years:
        description: '要处理的年份列表 (例如: 2018,2019,2020)，留空表示处理所有年份'
        required: false
        type: string
        default: ''
      matrix_count:
        description: '要创建的并行矩阵数量 (推荐: 10-40)'
        required: false
        type: number
        default: 20

jobs:
  list_files_and_create_tasks:
    runs-on: ubuntu-latest
    outputs:
      task_matrices: ${{ steps.list_files.outputs.task_matrices }}
      matrix_count: ${{ steps.list_files.outputs.matrix_count }}
      total_tasks: ${{ steps.list_files.outputs.total_tasks }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # 只安装必要的依赖用于任务拆分，不包含torch和embed等重型依赖
          pip install huggingface_hub polars toml numpy tqdm

      - name: List files, check embeddings, and create task matrices
        id: list_files
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          REPO_ID: ${{ inputs.repo_id || 'lyk/ArxivEmbedding' }}
          MATRIX_COUNT: ${{ inputs.matrix_count || 10 }}
          TARGET_YEARS: ${{ inputs.years || '' }} # Corrected: use inputs.years
          CONFIG_FILE: config.toml
          OUTPUT_DIR: matrix_tasks # Directory for task files
          THRESHOLD: 1e-7 # Keep threshold consistent if needed, though script has it hardcoded now
        run: | # Replace python shell with standard run block
          echo "Running task splitting script..."
          # Ensure output directory exists (script does this too, but good practice)
          mkdir -p $OUTPUT_DIR
          
          # Execute the script on a single line to avoid backslash issues
          python script/local_split_tasks.py --repo-id "$REPO_ID" --matrix-count "$MATRIX_COUNT" --years "$TARGET_YEARS" --config-file "$CONFIG_FILE" --output-dir "$OUTPUT_DIR" --max-tasks-per-matrix 10
            
          # Script's stdout/stderr will still be printed for logging.
          # No need for manual parsing or setting outputs here anymore.
          echo "Task splitting script finished. Outputs should be set directly."

      - name: Upload individual task files
        if: steps.list_files.outputs.matrix_count != '0' # Use string comparison
        uses: actions/upload-artifact@v4
        with:
          name: matrix-tasks # A single artifact containing all task files
          path: matrix_tasks/
          retention-days: 1 # Keep task files for 1 day

      - name: Upload task summary info
        uses: actions/upload-artifact@v4
        with:
          name: task-summary-info
          path: matrix_tasks/all_tasks_info.json
          retention-days: 1

      - name: Debug Outputs
        if: always() # Ensure this runs even if previous steps failed slightly
        run: |
          echo "Debug: total_tasks = ${{ steps.list_files.outputs.total_tasks }}"
          echo "Debug: matrix_count = ${{ steps.list_files.outputs.matrix_count }}"
          echo "Debug: task_matrices = ${{ steps.list_files.outputs.task_matrices }}"

  process_task_matrix:
    needs: list_files_and_create_tasks
    if: needs.list_files_and_create_tasks.outputs.matrix_count != '0' # Use string comparison
    runs-on: ubuntu-latest # Consider GPU runners if available: ubuntu-latest-gpu
    strategy:
      fail-fast: false # Continue other jobs even if one fails
      matrix:
        matrix_info: ${{ fromJson(needs.list_files_and_create_tasks.outputs.task_matrices) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Get pip cache dir
        id: pip-cache
        run: |
          echo "dir=$(pip cache dir)" >> $GITHUB_OUTPUT

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ${{ steps.pip-cache.outputs.dir }}
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Use --extra-index-url so pip checks PyPI *and* the PyTorch CPU index
          pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cpu
          # Add accelerate if using multi-gpu or specific optimizations
          # pip install accelerate
          # Add GPU specific libraries if using GPU runner
          # pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Example for CUDA 11.8

      - name: Download tasks for this matrix
        uses: actions/download-artifact@v4
        with:
          name: matrix-tasks # Name of the artifact containing task files
          path: matrix_tasks # Download to a local directory

      - name: Process Tasks using Python Script
        id: process_tasks
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
          REPO_ID: lyk/ArxivEmbedding
          MATRIX_ID: ${{ matrix.matrix_info.matrix_id }} # Get matrix_id from strategy
        run: |
          # Define matrix_id and task_file path based on the strategy
          MATRIX_ID=${{ matrix.matrix_info.matrix_id }}
          TASK_FILE="matrix_tasks/matrix_${MATRIX_ID}_tasks.json"
          OUTPUT_DIR="matrix_output_${MATRIX_ID}"

          # Create output directory
          mkdir -p $OUTPUT_DIR

          echo "Processing Matrix ID: $MATRIX_ID"
          echo "Task File: $TASK_FILE"
          echo "Output Directory: $OUTPUT_DIR"

          # Run the script on a single line
          python script/process_matrix_tasks.py --matrix-id $MATRIX_ID --task-file $TASK_FILE --output-dir $OUTPUT_DIR --config-file config.toml --repo-id $REPO_ID --batch-size 4 --engine torch --device cpu

      - name: Upload matrix output artifacts
        uses: actions/upload-artifact@v4
        with:
          name: matrix-output-${{ matrix.matrix_info.matrix_id }} # Use matrix_id from strategy
          path: matrix_output_${{ matrix.matrix_info.matrix_id }} # Use matrix_id from strategy
          retention-days: 5 # Keep artifacts for 5 days

  merge_and_upload:
    needs: [list_files_and_create_tasks, process_task_matrix]
    # Run only if the process_task_matrix jobs were triggered (i.e., matrix_count > 0)
    # and ran successfully, or if matrix_count was 0 initially.
    # We need to be careful here. Let's run it always if list_files ran,
    # but the inner script should handle the case where no artifacts are downloaded.
    if: always() && needs.list_files_and_create_tasks.result == 'success'
    runs-on: ubuntu-latest
    # This job runs once after all matrix jobs are done (or if none were needed)
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install polars huggingface_hub tqdm loguru numpy

      - name: Download all matrix output artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts # Download all artifacts into 'artifacts' directory
          
      - name: Collect and organize all embedding outputs
        run: |
          # Create a directory to collect all embedded files
          mkdir -p all_embedded_files
          
          # Find all embedded-*.parquet files across all matrix outputs
          find artifacts/matrix-output-* -name "embedded-*.parquet" -exec cp {} all_embedded_files/ \;
          
          # List what we found
          echo "Found the following embedded files:"
          ls -la all_embedded_files/

      - name: Merge Embeddings and Upload Script
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }} # Pass token as env var
          # REPO_ID is passed as argument
        run: |
          echo "Running merge and upload script..."
          # Run the script on a single line, using the new merge.py
          python script/merge.py --repo-id "lyk/ArxivEmbedding" --artifacts-dir "artifacts"

          echo "Merge and upload script finished."