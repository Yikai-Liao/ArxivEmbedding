name: Batch Generate Embeddings for Historical Data

on:
  workflow_dispatch: # Manual trigger
    inputs:
      years:
        description: '要处理的年份列表 (例如: 2018,2019,2020)，留空表示处理所有年份'
        required: false
        type: string
        default: ''
      matrix_count:
        description: '要创建的并行矩阵数量 (推荐: 10-40)'
        required: false
        type: number
        default: 20

jobs:
  list_files_and_create_tasks:
    runs-on: ubuntu-latest
    outputs:
      task_matrices: ${{ steps.create_task_matrices.outputs.task_matrices }}
      total_tasks: ${{ steps.create_task_matrices.outputs.total_tasks }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # 只安装必要的依赖用于任务拆分，不包含torch和embed等重型依赖
          pip install huggingface_hub polars toml

      - name: List Parquet files, Models and Create Task Matrices
        id: create_task_matrices
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }} # Needs repo write access
          REPO_ID: lyk/ArxivEmbedding # Your dataset ID
          MATRIX_COUNT: ${{ github.event.inputs.matrix_count || 20 }} # 使用输入参数设置矩阵数量，默认20
          TARGET_YEARS: ${{ github.event.inputs.years }}
        run: |
          import os
          import sys
          from huggingface_hub import HfApi, hf_hub_download
          import toml
          import json
          import math
          import polars as pl
          from tqdm import tqdm
          
          # 获取需要处理的年份列表（如果有指定）
          target_years_input = os.environ.get('TARGET_YEARS', '').strip()
          target_years = []
          if target_years_input:
              target_years = [year.strip() for year in target_years_input.split(',')]
              print(f"用户指定处理的年份: {target_years}")
          
          # List files from Hub
          api = HfApi()
          repo_files = api.list_repo_files(repo_id=os.environ['REPO_ID'], repo_type='dataset')
          # Filter for YYYY.parquet files (e.g., 2018.parquet, 2024.parquet)
          # Exclude current.parquet and non-parquet files
          year_files = [f for f in repo_files if f.endswith('.parquet') and f[:-len('.parquet')].isdigit()]
          
          # 如果指定了年份，只处理那些年份
          if target_years:
              year_files = [f for f in year_files if any(year in f for year in target_years)]
              
          print(f"Found yearly files to process: {year_files}")
          
          # 解析配置文件中的模型
          config = toml.load("config.toml")
          embedding_configs = {}
          
          # 打印配置文件结构以便调试
          print("配置文件结构:")
          for key, value in config.items():
              print(f"  {key}: {type(value)}")
              if isinstance(value, dict):
                  for subkey in value.keys():
                      print(f"    - {subkey}")
          
          # 正确处理[Embedding.XXX]格式
          if "Embedding" in config and isinstance(config["Embedding"], dict):
              embedding_section = config["Embedding"]
              for key, value in embedding_section.items():
                  embedding_configs[key] = value
                  print(f"找到模型配置: {key} -> {value}")
          
          model_keys = list(embedding_configs.keys())
          
          if not model_keys:
              print("错误: 无法从config.toml中找到模型配置。")
              sys.exit(1)
          
          print(f"Found model keys: {model_keys}")
          print(f"Models with configurations:")
          for k, v in embedding_configs.items():
              print(f"  - {k}: {v}")
          
          # 新的基于ID的任务分配方法
          # 收集所有年份文件中的ID
          all_ids = []
          year_id_map = {}  # 记录每个ID属于哪个年份文件

          print("开始下载并读取所有年份文件的ID...")
          for year_file in tqdm(year_files, desc="读取文件ID"):
              try:
                  # 下载年份文件
                  file_path = hf_hub_download(
                      repo_id=os.environ['REPO_ID'],
                      filename=year_file,
                      repo_type="dataset",
                      local_dir=".",
                      local_dir_use_symlinks=False
                  )
                  
                  # 只读取ID列，减少内存使用
                  df = pl.read_parquet(file_path, columns=["id"])
                  file_ids = df["id"].to_list()
                  
                  # 记录ID与年份文件的对应关系
                  for id_value in file_ids:
                      year_id_map[id_value] = year_file
                  
                  # 收集所有ID
                  all_ids.extend(file_ids)
                  
                  # 清理下载的文件
                  os.remove(file_path)
                  
                  print(f"已从 {year_file} 读取 {len(file_ids)} 个ID")
              except Exception as e:
                  print(f"读取 {year_file} 时出错: {e}")
          
          # 去重
          unique_ids = list(set(all_ids))
          print(f"Total unique IDs across all files: {len(unique_ids)}")
          
          # 计算每个模型处理的总任务数
          total_tasks = len(unique_ids) * len(model_keys)
          print(f"Total tasks (unique IDs × models): {total_tasks}")
          
          # 根据矩阵数量均匀分配ID
          target_matrix_count = int(os.environ['MATRIX_COUNT'])
          
          # 计算合理的矩阵数量
          if total_tasks < target_matrix_count:
              # 少任务时使用更少的矩阵
              if total_tasks <= 10:
                  actual_matrix_count = 1  # 非常少的任务，使用1个矩阵
              elif total_tasks <= 50:
                  actual_matrix_count = min(total_tasks // 10 + 1, 5)  # 适中的任务数，最多5个矩阵
              else:
                  actual_matrix_count = min(total_tasks // 20 + 1, 10)  # 较多任务，最多10个矩阵
              print(f"任务总数({total_tasks})少于请求的矩阵数量({target_matrix_count})，将使用{actual_matrix_count}个矩阵")
          else:
              actual_matrix_count = min(target_matrix_count, total_tasks)
          
          # 确保至少有一个矩阵
          actual_matrix_count = max(1, actual_matrix_count)
          
          # 每个矩阵处理的ID数量
          ids_per_matrix = math.ceil(len(unique_ids) / actual_matrix_count)
          
          # 创建任务矩阵
          task_matrices = []
          for i in range(actual_matrix_count):
              start_idx = i * ids_per_matrix
              end_idx = min((i + 1) * ids_per_matrix, len(unique_ids))
              
              if start_idx < len(unique_ids):
                  # 当前矩阵负责的ID
                  matrix_ids = unique_ids[start_idx:end_idx]
                  
                  # 创建模型和ID的任务组合
                  matrix_tasks = []
                  for id_value in matrix_ids:
                      year_file = year_id_map[id_value]
                      for model_key in model_keys:
                          matrix_tasks.append({
                              "id": id_value,
                              "year_file": year_file,
                              "model_key": model_key
                          })
                  
                  task_matrices.append({
                      "matrix_id": i,
                      "tasks": matrix_tasks
                  })
          
          # 检查分配情况
          actual_matrices = len(task_matrices)
          total_assigned_tasks = sum(len(matrix["tasks"]) for matrix in task_matrices)
          print(f"创建了 {actual_matrices} 个矩阵，分配了总计 {total_assigned_tasks} 个任务")
          
          # 输出任务矩阵信息
          for i, matrix in enumerate(task_matrices):
              task_count = len(matrix["tasks"])
              print(f"矩阵 {i}: {task_count} 个任务")
          
          # 保存任务矩阵到临时文件
          with open("task_matrices.json", "w") as f:
              json.dump(task_matrices, f)
          
          # 输出到GitHub Actions
          task_matrices_json = json.dumps(task_matrices)
          # 使用新版的输出语法
          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"task_matrices={task_matrices_json}\n")
              f.write(f"total_tasks={total_tasks}\n")
          
          # 上传任务矩阵作为参考
          with open("all_tasks.json", "w") as f:
              all_tasks_info = {
                  "total_unique_ids": len(unique_ids),
                  "total_models": len(model_keys),
                  "total_tasks": total_tasks,
                  "matrices_count": actual_matrices
              }
              json.dump(all_tasks_info, f)
        shell: python # Use python shell directly
          
      - name: Upload task definitions
        uses: actions/upload-artifact@v4
        with:
          name: task-definitions
          path: |
            task_matrices.json
            all_tasks.json

  process_task_matrix:
    needs: list_files_and_create_tasks
    runs-on: ubuntu-latest # Consider GPU runners if available: ubuntu-latest-gpu
    strategy:
      fail-fast: false # Continue other jobs even if one fails
      matrix:
        matrix_id: ${{ fromJson(needs.list_files_and_create_tasks.outputs.task_matrices) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install base requirements + embedding libs
          pip install -r requirements.txt
          pip install embed torch toml tqdm loguru huggingface_hub
          # Add accelerate if using multi-gpu or specific optimizations
          # pip install accelerate
          # Add GPU specific libraries if using GPU runner
          # pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Example for CUDA 11.8

      - name: Process Tasks in Matrix
        id: process_tasks
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
          REPO_ID: lyk/ArxivEmbedding
          MATRIX_ID: ${{ matrix.matrix_id.matrix_id }}
          MATRIX_TASKS: ${{ toJson(matrix.matrix_id.tasks) }}
        run: |
          import os
          import json
          import sys
          from pathlib import Path
          from collections import defaultdict
          from huggingface_hub import hf_hub_download
          from tqdm import tqdm
          import polars as pl
          from loguru import logger
          import subprocess
          import re
          
          logger.remove()
          logger.add(sys.stderr, level="INFO", format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
          
          # 解析分配给此矩阵的任务
          matrix_id = int(os.environ["MATRIX_ID"])
          tasks = json.loads(os.environ["MATRIX_TASKS"])
          
          logger.info(f"矩阵 {matrix_id} 处理 {len(tasks)} 个任务")
          
          # 按年份文件分组任务，以便高效下载和处理
          tasks_by_year_file = defaultdict(list)
          tasks_by_model = defaultdict(list)
          
          for task in tasks:
              year_file = task["year_file"]
              model_key = task["model_key"]
              id_value = task["id"]
              tasks_by_year_file[year_file].append(task)
              tasks_by_model[model_key].append(task)
          
          logger.info(f"任务涉及 {len(tasks_by_year_file)} 个不同年份文件和 {len(tasks_by_model)} 个不同模型")
          
          # 创建输出目录
          output_dir = Path(f"matrix_output_{matrix_id}")
          output_dir.mkdir(exist_ok=True)
          
          processed_tasks = []
          failed_tasks = []
          
          # 创建用于追踪已处理ID的集合
          processed_ids = set()
          
          # 按年份文件分组处理
          for year_file, year_tasks in tqdm(tasks_by_year_file.items(), desc=f"处理年份文件"):
              # 找出此年份文件需要处理的ID和模型
              ids_to_process = set(task["id"] for task in year_tasks)
              models_for_this_year = set(task["model_key"] for task in year_tasks)
              
              logger.info(f"处理 {year_file}: {len(ids_to_process)} 个ID, {len(models_for_this_year)} 个模型")
              
              # 下载整个年份文件
              try:
                  file_path = hf_hub_download(
                      repo_id=os.environ["REPO_ID"],
                      filename=year_file,
                      repo_type="dataset",
                      local_dir=".",
                      local_dir_use_symlinks=False
                  )
                  logger.info(f"下载 {year_file} 到 {file_path}")
                  
                  # 读取数据并过滤出需要处理的ID
                  df = pl.read_parquet(file_path)
                  filtered_df = df.filter(pl.col("id").is_in(ids_to_process))
                  
                  logger.info(f"从 {year_file} 中过滤出 {filtered_df.height} 条记录（总共 {df.height} 条）")
                  
                  # 如果过滤后没有记录，跳过处理
                  if filtered_df.height == 0:
                      logger.warning(f"{year_file} 中没有找到需要处理的ID，跳过处理")
                      continue
                  
                  # 保存过滤后的数据到临时文件
                  temp_file = f"temp_{matrix_id}_{year_file}"
                  filtered_df.write_parquet(temp_file)
                  logger.info(f"将过滤后的数据保存到临时文件 {temp_file}")
                  
                  # 为每个模型处理过滤后的数据
                  for model_key in models_for_this_year:
                      # 跳过已处理的组合
                      output_file = output_dir / f"embedded-{year_file}-{model_key}.parquet"
                      
                      logger.info(f"为 {year_file} 和模型 {model_key} 生成嵌入向量")
                      
                      try:
                          # 运行嵌入生成
                          cmd = [
                              "python", "script/generate_embeddings.py",
                              "--input-file", temp_file,
                              "--output-file", str(output_file),
                              "--model-key", model_key,
                              "--config-file", "config.toml",
                              "--batch-size", "32",  # 增大批处理大小
                              "--engine", "torch",
                              "--device", "cpu"  # 使用GPU请改为"cuda"
                          ]
                          
                          logger.info(f"运行命令: {' '.join(cmd)}")
                          process = subprocess.run(cmd, check=True, capture_output=True, text=True)
                          
                          if process.returncode == 0:
                              logger.info(f"成功为 {year_file} 和模型 {model_key} 生成嵌入向量")
                              # 记录每个成功处理的ID和模型组合
                              for id_value in filtered_df["id"].to_list():
                                  processed_tasks.append({
                                      "id": id_value,
                                      "year_file": year_file,
                                      "model_key": model_key,
                                      "output_file": str(output_file)
                                  })
                                  processed_ids.add(id_value)
                          else:
                              logger.error(f"命令返回非零状态码 {process.returncode}")
                              logger.error(f"STDOUT: {process.stdout}")
                              logger.error(f"STDERR: {process.stderr}")
                              # 记录失败的模型和年份组合
                              for id_value in filtered_df["id"].to_list():
                                  failed_tasks.append({
                                      "id": id_value,
                                      "year_file": year_file,
                                      "model_key": model_key,
                                      "error": f"处理错误: {process.stderr}"
                                  })
                      
                      except Exception as e:
                          logger.error(f"运行嵌入生成时出错: {e}")
                          for id_value in filtered_df["id"].to_list():
                              failed_tasks.append({
                                  "id": id_value,
                                  "year_file": year_file,
                                  "model_key": model_key,
                                  "error": str(e)
                              })
                  
                  # 清理临时文件
                  try:
                      if os.path.exists(temp_file):
                          os.remove(temp_file)
                  except Exception as e:
                      logger.warning(f"无法删除临时文件 {temp_file}: {e}")
              
              except Exception as e:
                  logger.error(f"下载或处理 {year_file} 时出错: {e}")
                  for task in year_tasks:
                      failed_tasks.append({
                          "id": task["id"],
                          "year_file": year_file,
                          "model_key": task["model_key"],
                          "error": f"下载失败: {str(e)}"
                      })
              
              finally:
                  # 清理下载的文件
                  try:
                      if 'file_path' in locals() and os.path.exists(file_path):
                          os.remove(file_path)
                  except Exception as e:
                      logger.warning(f"无法删除下载的文件 {file_path}: {e}")
          
          # 检查是否所有分配的ID都已处理
          assigned_ids = set(task["id"] for task in tasks)
          missed_ids = assigned_ids - processed_ids
          
          if missed_ids:
              logger.warning(f"有 {len(missed_ids)} 个ID未能成功处理: {list(missed_ids)[:10]}{'...' if len(missed_ids) > 10 else ''}")
          
          # 将结果写入JSON供artifacts使用
          with open(output_dir / "processed_tasks.json", "w") as f:
              json.dump(processed_tasks, f, indent=2)
              
          with open(output_dir / "failed_tasks.json", "w") as f:
              json.dump(failed_tasks, f, indent=2)
              
          # 摘要
          success_percent = 100 * len(processed_tasks) / len(tasks) if tasks else 0
          logger.info(f"矩阵 {matrix_id} 完成。成功: {len(processed_tasks)} ({success_percent:.1f}%), 失败: {len(failed_tasks)}")
          
          # 创建易于阅读的摘要文件
          with open(output_dir / "summary.txt", "w") as f:
              f.write(f"矩阵 {matrix_id} 摘要\n")
              f.write(f"==================\n")
              f.write(f"总任务数: {len(tasks)}\n")
              f.write(f"成功: {len(processed_tasks)} ({success_percent:.1f}%)\n")
              f.write(f"失败: {len(failed_tasks)}\n\n")
              
              if failed_tasks:
                  failure_by_reason = defaultdict(int)
                  for task in failed_tasks:
                      error_msg = task["error"]
                      # 简化错误消息以便分组
                      simplified_error = re.sub(r'\b[a-zA-Z0-9]{8,}\b', '[ID]', error_msg)
                      simplified_error = re.sub(r'\d+', '[NUM]', simplified_error)
                      failure_by_reason[simplified_error[:100]] += 1
                  
                  f.write("失败原因统计:\n")
                  for reason, count in sorted(failure_by_reason.items(), key=lambda x: x[1], reverse=True):
                      f.write(f"- {reason}... ({count} 次)\n")
                  
                  f.write("\n前10个失败任务:\n")
                  for idx, task in enumerate(failed_tasks[:10]):
                      f.write(f"{idx+1}. {task['year_file']} - {task['model_key']} - ID: {task['id']}: {task['error'][:100]}...\n")

      - name: Upload matrix output artifacts
        uses: actions/upload-artifact@v4
        with:
          name: matrix-output-${{ matrix.matrix_id.matrix_id }}
          path: matrix_output_${{ matrix.matrix_id.matrix_id }}
          retention-days: 5 # Keep artifacts for 5 days

  merge_and_upload:
    needs: [list_files_and_create_tasks, process_task_matrix]
    runs-on: ubuntu-latest
    # This job runs once after all matrix jobs are done
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install polars huggingface_hub tqdm loguru

      - name: Download all matrix output artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts # Download all artifacts into 'artifacts' directory
          
      - name: Collect and organize all embedding outputs
        run: |
          # Create a directory to collect all embedded files
          mkdir -p all_embedded_files
          
          # Find all embedded-*.parquet files across all matrix outputs
          find artifacts/matrix-output-* -name "embedded-*.parquet" -exec cp {} all_embedded_files/ \;
          
          # List what we found
          echo "Found the following embedded files:"
          ls -la all_embedded_files/

      - name: Merge Embeddings and Upload
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
          REPO_ID: lyk/ArxivEmbedding
        run: |
          import os
          import glob
          import polars as pl
          from huggingface_hub import HfApi, hf_hub_download, upload_file
          from tqdm import tqdm
          from loguru import logger
          import re

          repo_id = os.environ['REPO_ID']
          api = HfApi()

          # Identify unique year files from the embedded file patterns
          embedded_files = glob.glob("all_embedded_files/embedded-*.parquet")
          year_files_to_process = set()
          
          # Extract year from filenames like embedded-2020.parquet-model_key.parquet
          for file_path in embedded_files:
              filename = os.path.basename(file_path)
              # Extract the original year file name
              match = re.search(r'embedded-([0-9]+\.parquet)', filename)
              if match:
                  year_file = match.group(1)
                  year_files_to_process.add(year_file)

          logger.info(f"Found year files to process: {year_files_to_process}")

          for year_file in tqdm(list(year_files_to_process), desc="Merging and Uploading Years"):
              logger.info(f"--- Processing {year_file} ---")
              local_final_path = f"final_{year_file}"
              
              # Find all embedding files for this year
              embedding_files = [f for f in embedded_files if f"embedded-{year_file}-" in os.path.basename(f)]
              
              if not embedding_files:
                  logger.warning(f"No embedding artifacts found for {year_file}. Skipping.")
                  continue

              # Download the original file from the Hub to merge into
              try:
                  logger.info(f"Downloading original {year_file} from Hub...")
                  original_file_path = hf_hub_download(repo_id=repo_id, filename=year_file, repo_type='dataset', local_dir='.', local_dir_use_symlinks=False)
                  logger.info(f"Loading original file: {original_file_path}")
                  df_merged = pl.read_parquet(original_file_path)
                  logger.info(f"Original {year_file} shape: {df_merged.shape}")
                  os.remove(original_file_path) # Clean up downloaded original
              except Exception as e:
                  logger.error(f"Failed to download or load original {year_file} from Hub: {e}. Skipping merge for this year.")
                  continue

              # Merge embedding columns from artifacts
              merged_cols_count = 0
              for embed_file in embedding_files:
                  try:
                      logger.debug(f"Loading embedding file: {embed_file}")
                      df_embed = pl.read_parquet(embed_file)
                      # Find the embedding column (assume it's the model_key)
                      embed_cols = [col for col in df_embed.columns if col not in df_merged.columns and col != 'id']
                      
                      if not embed_cols:
                          logger.warning(f"No new embedding columns found in {embed_file}. Skipping.")
                          continue
                          
                      for embed_col_name in embed_cols:
                          logger.debug(f"Found embedding column '{embed_col_name}' in {embed_file}")

                          # Select only id and the embedding column
                          df_embed_subset = df_embed.select(['id', embed_col_name])

                          # Left join embedding column onto the main dataframe
                          if embed_col_name not in df_merged.columns:
                               df_merged = df_merged.join(df_embed_subset, on='id', how='left')
                               logger.info(f"Merged column '{embed_col_name}'. New shape: {df_merged.shape}")
                               merged_cols_count += 1
                          else:
                               logger.warning(f"Column '{embed_col_name}' already exists in the merged DataFrame. Skipping merge from {embed_file}.")

                  except Exception as e:
                      logger.error(f"Failed to process or merge embedding file {embed_file}: {e}")

              if merged_cols_count > 0:
                  # Save locally before upload
                  try:
                      logger.info(f"Saving final merged file locally: {local_final_path}")
                      df_merged.write_parquet(local_final_path, compression='zstd')

                      # Upload the final merged file
                      logger.info(f"Uploading final {year_file} to Hub...")
                      upload_file(
                          path_or_fileobj=local_final_path,
                          path_in_repo=year_file, # Overwrite the file in the repo root
                          repo_id=repo_id,
                          repo_type="dataset",
                          commit_message=f"Add/Update embeddings for {year_file}"
                      )
                      logger.info(f"Successfully uploaded updated {year_file}.")
                  except Exception as e:
                      logger.error(f"Failed to save or upload final {year_file}: {e}")
                  finally:
                      if os.path.exists(local_final_path):
                          os.remove(local_final_path) # Clean up local final file
              else:
                  logger.warning(f"No new embedding columns were merged for {year_file}. Nothing to upload.")

          logger.info("--- Merge and Upload Job Finished ---")
        shell: python