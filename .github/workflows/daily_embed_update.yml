name: Daily Fetch, Embed, and Update Arxiv Data

on:
  workflow_dispatch: # Manual trigger
  schedule:
    - cron: '0 5 * * *' # Run daily at 5:00 UTC

jobs:
  fetch_rss:
    runs-on: ubuntu-latest
    outputs:
      current_file_path: data/current.parquet
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt # Assuming requirements.txt has feedparser, polars, toml, loguru

      - name: Fetch Arxiv RSS data
        run: python script/fetch_arxiv_rss.py

      - name: Upload current.parquet artifact
        uses: actions/upload-artifact@v4
        with:
          name: current-data
          path: data/current.parquet
          retention-days: 3

  generate_embeddings_current:
    needs: fetch_rss
    runs-on: ubuntu-latest # Consider GPU runners
    outputs:
      embedded_current_file: embedded-current.parquet # Output path for the final merged current file
    strategy:
      fail-fast: false
      matrix:
        model_key: ${{ fromJson(needs.list_files.outputs.models || '[]') }} # Get models dynamically if list_files runs, or define statically
        # If list_files doesn't run in this workflow, define models statically:
        # model_key: ['jasper_v1', 'conan_v1'] # Example: Define model keys directly
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install sentence-transformers torch toml tqdm loguru huggingface_hub polars

      - name: Download current.parquet artifact
        uses: actions/download-artifact@v4
        with:
          name: current-data
          path: . # Download to current directory

      - name: Generate Embeddings for current data
        run: |
          python script/generate_embeddings.py \
            --input-file "current.parquet" \
            --output-file "embedded-current-${{ matrix.model_key }}.parquet" \
            --model-key "${{ matrix.model_key }}" \
            --config-file "config.toml" \
            --batch-size 32 # Smaller batch size might be okay for daily updates
            # --device cuda # Uncomment if using GPU runner

      - name: Upload individual embedded current file artifact
        uses: actions/upload-artifact@v4
        with:
          name: embedded-current-artifacts # Collect all model artifacts
          path: "embedded-current-${{ matrix.model_key }}.parquet"
          retention-days: 3

  merge_current_embeddings:
      needs: generate_embeddings_current
      runs-on: ubuntu-latest
      outputs:
        final_current_embedded_file: final_current_embedded.parquet
      steps:
        - name: Checkout code # Needed for config.toml potentially
          uses: actions/checkout@v4

        - name: Set up Python
          uses: actions/setup-python@v5
          with:
            python-version: '3.10'

        - name: Install dependencies
          run: |
            python -m pip install --upgrade pip
            pip install polars loguru

        - name: Download embedded current artifacts
          uses: actions/download-artifact@v4
          with:
            name: embedded-current-artifacts
            path: current_artifacts # Download all model artifacts here

        - name: Merge current embeddings
          id: merge
          run: |
            import glob
            import polars as pl
            import os
            from loguru import logger

            artifact_files = glob.glob("current_artifacts/embedded-current-*.parquet")
            if not artifact_files:
                logger.error("No embedded current artifacts found to merge.")
                exit(1) # Fail the job

            # Use the first file as the base
            base_file = artifact_files[0]
            logger.info(f"Using {base_file} as base for merging current embeddings.")
            df_merged = pl.read_parquet(base_file)

            # Merge other files
            for embed_file in artifact_files[1:]:
                try:
                    logger.debug(f"Loading embedding file: {embed_file}")
                    df_embed = pl.read_parquet(embed_file)
                    embed_col_name = [col for col in df_embed.columns if col not in df_merged.columns and col != 'id'][0]
                    logger.debug(f"Found embedding column '{embed_col_name}' in {embed_file}")

                    df_embed_subset = df_embed.select(['id', embed_col_name])
                    if embed_col_name not in df_merged.columns:
                        df_merged = df_merged.join(df_embed_subset, on='id', how='left')
                        logger.info(f"Merged column '{embed_col_name}'. New shape: {df_merged.shape}")
                    else:
                        logger.warning(f"Column '{embed_col_name}' already exists. Skipping merge from {embed_file}.")
                except Exception as e:
                    logger.error(f"Failed to process or merge {embed_file}: {e}")

            output_filename = "final_current_embedded.parquet"
            logger.info(f"Saving final merged current data to {output_filename}")
            df_merged.write_parquet(output_filename, compression='zstd')
            print(f"::set-output name=final_current_embedded_file::{output_filename}")

          shell: python

        - name: Upload final merged current data artifact
          uses: actions/upload-artifact@v4
          with:
            name: final-current-embedded
            path: ${{ steps.merge.outputs.final_current_embedded_file }}
            retention-days: 3

  update_yearly_files:
    needs: merge_current_embeddings
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Need dependencies for rss2hg.py + huggingface_hub
          pip install -r requirements.txt
          pip install huggingface_hub loguru polars

      - name: Download final merged current data artifact
        uses: actions/download-artifact@v4
        with:
          name: final-current-embedded
          path: data # Download to data directory

      - name: Rename final embedded file
        run: mv data/${{ needs.merge_current_embeddings.outputs.final_current_embedded_file }} data/current_embedded.parquet

      - name: Run Yearly Merge Script (using embedded data)
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
          # Override the input file used by rss2hg.py via environment variable or modify the script
          CURRENT_DATA_INPUT_FILE: data/current_embedded.parquet # Custom env var
        run: |
          # We need to modify rss2hg.py or create a new script
          # For now, let's assume rss2hg.py is modified to accept an env var for the input file
          # Or, we modify it directly here if simple enough:
          sed -i "s/CURRENT_DATA_FILE = \"current.parquet\"/CURRENT_DATA_FILE = \"current_embedded.parquet\"/" script/rss2hg.py
          # Make sure the DATE_COLUMN_FOR_YEAR is correct for your embedded file ('created' or 'date')
          # sed -i "s/DATE_COLUMN_FOR_YEAR = \"created\"/DATE_COLUMN_FOR_YEAR = \"date\"/" script/rss2hg.py # If needed

          echo "Running rss2hg.py with embedded data..."
          python script/rss2hg.py
