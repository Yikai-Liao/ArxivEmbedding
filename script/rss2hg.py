# script/merge_updates_by_year.py
import os
import polars as pl
from huggingface_hub import HfApi, hf_hub_download, upload_file
from huggingface_hub.utils import RepositoryNotFoundError, EntryNotFoundError
from loguru import logger
import sys
import toml # Import toml to read config
import numpy as np # Import numpy for zero arrays

# Configure Loguru
logger.remove()
logger.add(sys.stderr, level="INFO", format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>")

# --- Configuration ---
HUB_DATASET_ID = "lyk/ArxivEmbedding" # Your dataset ID
LOCAL_DATA_DIR = "data"
CURRENT_DATA_FILE = "current.parquet" # File generated by fetch_arxiv_rss.py
DATE_COLUMN_FOR_YEAR = "created" # Column to determine the year ('created' or 'date')

# --- Load Model Config for Dimensions ---
try:
    config_path = "config.toml" # Assuming config.toml is in the root directory
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found at {config_path}")
    config = toml.load(config_path)
    embedding_configs = config.get("Embedding", {})
    if not embedding_configs:
        raise ValueError("No [Embedding] section found in config.toml")

    MODEL_DIMS = {}
    for key, value in embedding_configs.items():
        if isinstance(value, dict) and "dimension" in value:
            MODEL_DIMS[key] = value["dimension"]
        else:
            logger.warning(f"Could not find 'dimension' for model '{key}' in config.toml. Skipping zero-fill for this model.")
    logger.info(f"Loaded embedding dimensions: {MODEL_DIMS}")

except Exception as e:
    logger.error(f"Failed to load or parse model dimensions from config.toml: {e}")
    # Decide how to proceed - exit or continue without zero-fill? Exit is safer.
    sys.exit(1)
# --- End Model Config Loading ---

def process_year_group(year, new_data_for_year_pl, schema):
    """
    Downloads, merges, deduplicates, and uploads data for a specific year.

    Args:
        year (int): The year being processed.
        new_data_for_year_pl (pl.DataFrame): DataFrame containing new/updated records for this year.
        schema (dict): The expected schema for the DataFrame to ensure consistency.

    Returns:
        bool: True if processing and upload were successful (or nothing needed), False otherwise.
    """
    target_filename = f"{year}.parquet"
    temp_download_path = os.path.join(LOCAL_DATA_DIR, f"{target_filename}.temp_download")
    local_output_path = os.path.join(LOCAL_DATA_DIR, target_filename) # Final path for upload

    logger.info(f"--- Processing Year: {year} ---")
    logger.info(f"Target file on Hub: {target_filename}")
    logger.info(f"Number of new/updated records for this year: {new_data_for_year_pl.height}")

    # --- Download Existing Yearly Data from Hub ---
    existing_data_pl = None
    try:
        logger.info(f"Attempting to download existing '{target_filename}' from Hub repo '{HUB_DATASET_ID}'...")
        # Use hf_hub_download which handles caching and returns the path
        downloaded_path = hf_hub_download(
            repo_id=HUB_DATASET_ID,
            filename=target_filename,
            repo_type="dataset",
            local_dir=LOCAL_DATA_DIR,
            local_dir_use_symlinks=False,
            # Using cache might be fine, but force download for safety if needed:
            # force_download=True,
            # cache_dir=None # Can force download to specific spot outside cache
        )
        # Rename downloaded file to avoid conflict during write if using same name
        os.rename(downloaded_path, temp_download_path)

        logger.info(f"Successfully downloaded '{target_filename}' to '{temp_download_path}'. Loading...")
        existing_data_pl = pl.read_parquet(temp_download_path)
        logger.info(f"Loaded {existing_data_pl.height} existing records from Hub for year {year}.")

    except EntryNotFoundError:
        logger.info(f"'{target_filename}' not found on the Hub for year {year}. Will create it.")
        # Create empty DataFrame with the correct schema
        existing_data_pl = pl.DataFrame(schema=schema)
    except RepositoryNotFoundError:
        logger.error(f"Hub repository '{HUB_DATASET_ID}' not found. Cannot proceed for year {year}.")
        return False # Indicate failure for this year
    except Exception as e:
        logger.error(f"Failed to download or read existing data for year {year} from Hub: {e}")
        # Clean up potentially corrupted download
        if os.path.exists(temp_download_path):
            try:
                os.remove(temp_download_path)
            except OSError as rm_err:
                logger.warning(f"[{year}] Could not remove temporary download file {temp_download_path}: {rm_err}")
        return False # Indicate failure for this year

    # --- Merge, Align Schema, and Deduplicate ---
    try:
        logger.info(f"Combining existing and new data for year {year}...")
        # Ensure schemas are compatible before concatenating using the provided schema
        all_columns = list(schema.keys()) # Use the expected full schema

        # Add missing columns (filled with null or zero vector) to existing data
        for col, dtype in schema.items():
            if col not in existing_data_pl.columns:
                if col in MODEL_DIMS:
                    dim = MODEL_DIMS[col]
                    target_dtype = pl.Array(pl.Float32, dim)
                    zero_vec = np.zeros((dim,), dtype=np.float32)
                    logger.warning(f"[{year}] Adding missing embedding column '{col}' to existing data (filling with zeros). Type: {target_dtype}")
                    existing_data_pl = existing_data_pl.with_columns(pl.lit(zero_vec).cast(target_dtype).alias(col)) # Cast might be redundant but safe
                else:
                    logger.warning(f"[{year}] Adding missing non-embedding column '{col}' to existing data (filling with null). Type: {dtype}")
                    existing_data_pl = existing_data_pl.with_columns(pl.lit(None).cast(dtype).alias(col))

        # Add missing columns (filled with null or zero vector) to new data
        for col, dtype in schema.items():
            if col not in new_data_for_year_pl.columns:
                 if col in MODEL_DIMS:
                    dim = MODEL_DIMS[col]
                    target_dtype = pl.Array(pl.Float32, dim)
                    zero_vec = np.zeros((dim,), dtype=np.float32)
                    logger.warning(f"[{year}] Adding missing embedding column '{col}' to new data (filling with zeros). Type: {target_dtype}")
                    new_data_for_year_pl = new_data_for_year_pl.with_columns(pl.lit(zero_vec).cast(target_dtype).alias(col))
                 else:
                    logger.warning(f"[{year}] Adding missing non-embedding column '{col}' to new data (filling with null). Type: {dtype}")
                    new_data_for_year_pl = new_data_for_year_pl.with_columns(pl.lit(None).cast(dtype).alias(col))

        # Ensure consistent column order and types based on the schema before combining
        # Casting here ensures alignment even if types were slightly off before
        cast_expressions_existing = []
        cast_expressions_new = []
        for col, dtype in schema.items():
             if col in MODEL_DIMS and col in existing_data_pl.columns and existing_data_pl[col].dtype != dtype:
                  logger.warning(f"[{year}] Casting existing column '{col}' to expected type {dtype}")
                  cast_expressions_existing.append(pl.col(col).cast(dtype))
             if col in MODEL_DIMS and col in new_data_for_year_pl.columns and new_data_for_year_pl[col].dtype != dtype:
                  logger.warning(f"[{year}] Casting new column '{col}' to expected type {dtype}")
                  cast_expressions_new.append(pl.col(col).cast(dtype))
        
        if cast_expressions_existing:
             existing_data_pl = existing_data_pl.with_columns(cast_expressions_existing)
        if cast_expressions_new:
             new_data_for_year_pl = new_data_for_year_pl.with_columns(cast_expressions_new)
             
        existing_data_pl = existing_data_pl.select(all_columns)
        new_data_for_year_pl = new_data_for_year_pl.select(all_columns)

        # Concatenate
        combined_pl = pl.concat([existing_data_pl, new_data_for_year_pl], how="vertical_relaxed") # vertical_relaxed is safer
        logger.info(f"[{year}] Combined data has {combined_pl.height} records before deduplication.")

        # Deduplicate based on 'id', keeping the last entry
        if 'id' in combined_pl.columns:
            final_pl = combined_pl.unique(subset=['id'], keep='last', maintain_order=False)
            logger.info(f"[{year}] Deduplicated data has {final_pl.height} records.")
        else:
            logger.warning(f"[{year}] No 'id' column found for deduplication. Skipping.")
            final_pl = combined_pl

        # Sort by ID (optional, but good practice for consistency)
        if 'id' in final_pl.columns:
             final_pl = final_pl.sort('id')

    except Exception as e:
        logger.error(f"[{year}] Error during data merging or deduplication: {e}")
        # Clean up download before returning
        if os.path.exists(temp_download_path):
            try:
                os.remove(temp_download_path)
            except OSError as rm_err:
                logger.warning(f"[{year}] Could not remove temporary download file {temp_download_path}: {rm_err}")
        return False # Indicate failure

    # --- Save Merged Data Locally (Temporary) and Upload ---
    success = False
    if final_pl.height > 0:
        try:
            logger.info(f"[{year}] Saving final merged data locally to {local_output_path}...")
            final_pl.write_parquet(local_output_path, compression='zstd', use_pyarrow=True) # Consider specifying pyarrow

            logger.info(f"[{year}] Uploading {local_output_path} to Hub repo '{HUB_DATASET_ID}' as '{target_filename}'...")
            upload_file(
                path_or_fileobj=local_output_path,
                path_in_repo=target_filename,
                repo_id=HUB_DATASET_ID,
                repo_type="dataset",
                commit_message=f"Update {target_filename} with {new_data_for_year_pl.height} new/updated records"
            )
            logger.info(f"[{year}] Successfully uploaded '{target_filename}' to the Hub.")
            success = True

        except Exception as e:
            logger.error(f"[{year}] Error saving locally or uploading merged data: {e}")
            success = False # Ensure failure is recorded
        finally:
            # Clean up temporary downloaded file and the final local file after upload attempt
            if os.path.exists(temp_download_path):
                try:
                    os.remove(temp_download_path)
                    logger.debug(f"[{year}] Cleaned up temporary download file: {temp_download_path}")
                except OSError as rm_err:
                     logger.warning(f"[{year}] Could not remove temporary download file {temp_download_path}: {rm_err}")
            if os.path.exists(local_output_path):
                 try:
                    os.remove(local_output_path)
                    logger.debug(f"[{year}] Cleaned up final local file: {local_output_path}")
                 except OSError as rm_err:
                     logger.warning(f"[{year}] Could not remove final local file {local_output_path}: {rm_err}")
    else:
        logger.info(f"[{year}] Final DataFrame for year {year} is empty after processing. Nothing to upload.")
        success = True # Considered success as there's nothing to do

        # Clean up temporary downloaded file even if final DF is empty
        if os.path.exists(temp_download_path):
            try:
                os.remove(temp_download_path)
                logger.debug(f"[{year}] Cleaned up temporary download file: {temp_download_path}")
            except OSError as rm_err:
                 logger.warning(f"[{year}] Could not remove temporary download file {temp_download_path}: {rm_err}")

    return success


def main():
    """
    Main function to orchestrate the merging and uploading process.
    Loads current data, extracts year, groups by year, and processes each group.
    """
    logger.info("--- Starting Yearly Merge & Update Script ---")

    local_current_path = os.path.join(LOCAL_DATA_DIR, CURRENT_DATA_FILE)

    # --- Load New Data from current.parquet ---
    if not os.path.exists(local_current_path):
        logger.error(f"Local current data file not found: {local_current_path}. Run fetch script first.")
        return 1 # Exit code 1 indicates error

    try:
        logger.info(f"Loading data from {local_current_path}...")
        current_data_pl = pl.read_parquet(local_current_path)
        logger.info(f"Loaded {current_data_pl.height} records.")
        if current_data_pl.height == 0:
            logger.info("No new records found in current.parquet. Nothing to merge.")
            return 0 # Exit code 0 indicates success (nothing to do)
        if DATE_COLUMN_FOR_YEAR not in current_data_pl.columns:
             logger.error(f"Required date column '{DATE_COLUMN_FOR_YEAR}' not found in {local_current_path}. Cannot determine year.")
             return 1
    except Exception as e:
        logger.error(f"Failed to read local current data file {local_current_path}: {e}")
        return 1

    # --- Extract Year using Polars Native Functions ---
    logger.info(f"Extracting year from '{DATE_COLUMN_FOR_YEAR}' column...")

    # Define the expected datetime format. IMPORTANT: Adjust if needed!
    # %Y-%m-%dT%H:%M:%SZ matches 'YYYY-MM-DDTHH:MM:SSZ' (ISO 8601 variant)
    # Other examples: '%Y-%m-%d', '%m/%d/%Y %H:%M:%S'
    DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%SZ"

    try:
        # Add a temporary '__year__' column using Polars native functions for efficiency
        current_data_pl = current_data_pl.with_columns(
            pl.col(DATE_COLUMN_FOR_YEAR)
            .str.strptime(
                dtype=pl.Datetime,      # Parse to Datetime type first
                format=DATETIME_FORMAT, # Specify the exact format of the string
                strict=False            # Return null if parsing fails for a row, don't raise Error
            )
            .dt.year()                  # Extract the year component (returns Int32)
            .alias("__year__")          # Name the new temporary column
        )
    except Exception as e:
        # Catch potential errors during expression building or execution
        logger.error(f"Failed to create '__year__' column from '{DATE_COLUMN_FOR_YEAR}': {e}")
        logger.error(f"Please ensure the column contains strings and the DATETIME_FORMAT ('{DATETIME_FORMAT}') matches the actual data format.")
        return 1 # Exit with error

    # --- Filter and Group Data by Year ---
    # Separate data with valid year from those where parsing failed
    valid_year_data = current_data_pl.filter(pl.col("__year__").is_not_null())
    invalid_year_data = current_data_pl.filter(pl.col("__year__").is_null())

    # Log warnings for skipped records
    if invalid_year_data.height > 0:
        logger.warning(f"Found {invalid_year_data.height} records with invalid/missing format in '{DATE_COLUMN_FOR_YEAR}'. These records will be skipped.")
        log_cols = ['id', DATE_COLUMN_FOR_YEAR] if 'id' in invalid_year_data.columns else [DATE_COLUMN_FOR_YEAR]
        try:
             logger.warning(f"Examples of skipped '{DATE_COLUMN_FOR_YEAR}' values:\n{invalid_year_data.select(log_cols).head(5)}")
        except Exception as log_e:
             logger.warning(f"(Could not log example details: {log_e})")


    if valid_year_data.height == 0:
        logger.info("No records with valid year information found after parsing. Nothing to process.")
        return 0 # Exit successfully

    # Get the unique years present in the valid data
    try:
        unique_years = valid_year_data.get_column("__year__").unique().sort().to_list()
        logger.info(f"Found new/updated data for years: {unique_years}")
    except Exception as e:
        logger.error(f"Could not retrieve unique years from '__year__' column: {e}")
        return 1

    # --- Process Each Year Group ---
    overall_success = True
    # Define the full schema based on the data that has valid years
    # This schema will be used to ensure consistency when creating empty DFs or merging
    full_schema = valid_year_data.drop("__year__").schema

    for year in unique_years:
        # Filter the DataFrame for the current year and drop the temporary year column
        data_for_year_pl = valid_year_data.filter(pl.col("__year__") == year).drop("__year__")

        if data_for_year_pl.height > 0:
             if not process_year_group(year, data_for_year_pl, full_schema):
                 overall_success = False # Mark failure if any year fails
                 logger.error(f"Processing failed for year {year}.")
        else:
             logger.info(f"Year {year} resulted in an empty DataFrame after filtering. Skipping.")


    # --- Final Summary ---
    if overall_success:
        logger.info("--- Yearly Merge & Update Script finished successfully for all processed years ---")
        return 0 # Success
    else:
        logger.error("--- Yearly Merge & Update Script finished with errors for one or more years ---")
        return 1 # Failure

if __name__ == "__main__":
    # Ensure data directory exists before starting
    os.makedirs(LOCAL_DATA_DIR, exist_ok=True)
    exit_code = main()
    sys.exit(exit_code)