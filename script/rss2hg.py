# script/merge_updates_by_year.py
import os
import polars as pl
from huggingface_hub import HfApi, hf_hub_download, upload_file
from huggingface_hub.utils import RepositoryNotFoundError, EntryNotFoundError
from loguru import logger
import sys
from collections import defaultdict

# Configure Loguru
logger.remove()
logger.add(sys.stderr, level="INFO", format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>")

# --- Configuration ---
HUB_DATASET_ID = "lyk/ArxivEmbedding" # Your dataset ID
LOCAL_DATA_DIR = "data"
CURRENT_DATA_FILE = "current.parquet" # File generated by fetch_arxiv_rss.py
DATE_COLUMN_FOR_YEAR = "created" # Column to determine the year ('created' or 'date')

def get_year_from_date_str(date_str):
    """Extracts year from YYYY-MM-DDTHH:MM:SSZ format."""
    if not date_str or len(date_str) < 4:
        return None
    try:
        return int(date_str[:4])
    except ValueError:
        return None

def process_year_group(year, new_data_for_year_pl, schema):
    """Downloads, merges, deduplicates, and uploads data for a specific year."""
    target_filename = f"{year}.parquet"
    temp_download_path = os.path.join(LOCAL_DATA_DIR, f"{target_filename}.temp_download")
    local_output_path = os.path.join(LOCAL_DATA_DIR, target_filename) # Final path for upload

    logger.info(f"--- Processing Year: {year} ---")
    logger.info(f"Target file on Hub: {target_filename}")
    logger.info(f"Number of new/updated records for this year: {new_data_for_year_pl.height}")

    # --- Download Existing Yearly Data from Hub ---
    existing_data_pl = None
    try:
        logger.info(f"Attempting to download existing '{target_filename}' from Hub repo '{HUB_DATASET_ID}'...")
        hf_hub_download(
            repo_id=HUB_DATASET_ID,
            filename=target_filename,
            repo_type="dataset",
            local_dir=LOCAL_DATA_DIR,
            local_dir_use_symlinks=False,
            cache_dir=None # Ensure fresh download
        )
        downloaded_path = os.path.join(LOCAL_DATA_DIR, target_filename)
        os.rename(downloaded_path, temp_download_path) # Rename to avoid conflicts

        logger.info(f"Successfully downloaded '{target_filename}' to '{temp_download_path}'. Loading...")
        existing_data_pl = pl.read_parquet(temp_download_path)
        logger.info(f"Loaded {existing_data_pl.height} existing records from Hub for year {year}.")

    except EntryNotFoundError:
        logger.info(f"'{target_filename}' not found on the Hub for year {year}. Will create it.")
        # Create empty DataFrame with the full schema derived from new data
        existing_data_pl = pl.DataFrame(schema=schema)
    except RepositoryNotFoundError:
        logger.error(f"Hub repository '{HUB_DATASET_ID}' not found. Cannot proceed for year {year}.")
        return False # Indicate failure for this year
    except Exception as e:
        logger.error(f"Failed to download or read existing data for year {year} from Hub: {e}")
        return False # Indicate failure for this year

    # --- Merge, Align Schema, and Deduplicate ---
    try:
        logger.info(f"Combining existing and new data for year {year}...")
        # Ensure schemas are compatible before concatenating
        all_columns = set(existing_data_pl.columns) | set(new_data_for_year_pl.columns)

        # Add missing columns filled with null to existing data
        for col in all_columns:
            if col not in existing_data_pl.columns:
                logger.warning(f"[{year}] Adding missing column '{col}' to existing data (filling with null).")
                existing_data_pl = existing_data_pl.with_columns(pl.lit(None).alias(col).cast(new_data_for_year_pl[col].dtype))

        # Add missing columns filled with null to new data
        for col in all_columns:
            if col not in new_data_for_year_pl.columns:
                logger.warning(f"[{year}] Adding missing column '{col}' to new data (filling with null).")
                new_data_pl = new_data_for_year_pl.with_columns(pl.lit(None).alias(col).cast(existing_data_pl[col].dtype))

        # Ensure consistent column order
        ordered_cols = sorted(list(all_columns))
        existing_data_pl = existing_data_pl.select(ordered_cols)
        new_data_for_year_pl = new_data_for_year_pl.select(ordered_cols)

        # Concatenate
        combined_pl = pl.concat([existing_data_pl, new_data_for_year_pl], how="vertical")
        logger.info(f"[{year}] Combined data has {combined_pl.height} records before deduplication.")

        # Deduplicate based on 'id', keeping the last entry
        if 'id' in combined_pl.columns:
            final_pl = combined_pl.unique(subset=['id'], keep='last', maintain_order=False)
            logger.info(f"[{year}] Deduplicated data has {final_pl.height} records.")
        else:
            logger.warning(f"[{year}] No 'id' column found for deduplication. Skipping.")
            final_pl = combined_pl

        # Sort by ID (optional)
        if 'id' in final_pl.columns:
             final_pl = final_pl.sort('id')

    except Exception as e:
        logger.error(f"[{year}] Error during data merging or deduplication: {e}")
        # Clean up download before returning
        if os.path.exists(temp_download_path):
            os.remove(temp_download_path)
        return False # Indicate failure

    # --- Save Merged Data Locally (Temporary) and Upload ---
    success = False
    if final_pl.height > 0:
        try:
            logger.info(f"[{year}] Saving final merged data locally to {local_output_path}...")
            final_pl.write_parquet(local_output_path, compression='zstd')

            logger.info(f"[{year}] Uploading {local_output_path} to Hub repo '{HUB_DATASET_ID}' as '{target_filename}'...")
            upload_file(
                path_or_fileobj=local_output_path,
                path_in_repo=target_filename,
                repo_id=HUB_DATASET_ID,
                repo_type="dataset",
                commit_message=f"Update {target_filename} with {new_data_for_year_pl.height} new/updated records"
            )
            logger.info(f"[{year}] Successfully uploaded '{target_filename}' to the Hub.")
            success = True

        except Exception as e:
            logger.error(f"[{year}] Error saving locally or uploading merged data: {e}")
            success = False # Ensure failure is recorded
        finally:
            # Clean up temporary downloaded file and the final local file
            if os.path.exists(temp_download_path):
                os.remove(temp_download_path)
                logger.debug(f"[{year}] Cleaned up temporary file: {temp_download_path}")
            if os.path.exists(local_output_path):
                 os.remove(local_output_path)
                 logger.debug(f"[{year}] Cleaned up final local file: {local_output_path}")
    else:
        logger.info(f"[{year}] Final DataFrame is empty. Nothing to upload.")
        success = True # Considered success as there's nothing to do

    return success


def main():
    logger.info("--- Starting Yearly Merge & Update Script ---")

    local_current_path = os.path.join(LOCAL_DATA_DIR, CURRENT_DATA_FILE)

    # --- Load New Data from current.parquet ---
    if not os.path.exists(local_current_path):
        logger.error(f"Local current data file not found: {local_current_path}. Did fetch_arxiv_rss.py run?")
        return 1

    try:
        current_data_pl = pl.read_parquet(local_current_path)
        logger.info(f"Loaded {current_data_pl.height} records from {local_current_path}")
        if current_data_pl.height == 0:
            logger.info("No records found in current.parquet. Nothing to merge.")
            return 0 # Exit successfully
        if DATE_COLUMN_FOR_YEAR not in current_data_pl.columns:
             logger.error(f"Required date column '{DATE_COLUMN_FOR_YEAR}' not found in {local_current_path}. Cannot determine year.")
             return 1
    except Exception as e:
        logger.error(f"Failed to read local current data file {local_current_path}: {e}")
        return 1

    # --- Group Data by Year ---
    logger.info(f"Grouping data by year based on '{DATE_COLUMN_FOR_YEAR}' column...")
    grouped_data = defaultdict(list)
    years_found = set()

    # Add a year column temporarily for easier grouping
    current_data_pl = current_data_pl.with_columns(
        pl.col(DATE_COLUMN_FOR_YEAR).apply(get_year_from_date_str, return_dtype=pl.Int64).alias("__year__")
    )

    # Filter out rows where year couldn't be determined
    valid_year_data = current_data_pl.filter(pl.col("__year__").is_not_null())
    invalid_year_data = current_data_pl.filter(pl.col("__year__").is_null())

    if invalid_year_data.height > 0:
        logger.warning(f"Found {invalid_year_data.height} records with invalid or missing '{DATE_COLUMN_FOR_YEAR}' data. These records will be skipped:")
        # Log some details of skipped records if needed
        # logger.warning(invalid_year_data.select(['id', DATE_COLUMN_FOR_YEAR]).head(5))

    if valid_year_data.height == 0:
        logger.info("No records with valid year information found. Nothing to process.")
        return 0

    # Group by the temporary year column
    grouped_by_year_pl = valid_year_data.group_by("__year__")

    # --- Process Each Year Group ---
    overall_success = True
    full_schema = valid_year_data.schema # Use schema from valid data

    for year_group in grouped_by_year_pl:
        year = year_group[0] # The year value from the group_by key
        data_for_year_pl = grouped_by_year_pl.get_group(year_group).drop("__year__") # Get the group's data and remove temp column

        if not process_year_group(year, data_for_year_pl, full_schema):
            overall_success = False # Mark failure if any year fails

    # --- Final Summary ---
    if overall_success:
        logger.info("--- Yearly Merge & Update Script finished successfully for all processed years ---")
        return 0
    else:
        logger.error("--- Yearly Merge & Update Script finished with errors for one or more years ---")
        return 1 # Indicate failure

if __name__ == "__main__":
    # Ensure data directory exists
    os.makedirs(LOCAL_DATA_DIR, exist_ok=True)
    sys.exit(main())
